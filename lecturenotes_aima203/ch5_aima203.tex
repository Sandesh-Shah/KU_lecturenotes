\documentclass[aima203_lecturenotes_ku.tex]{subfiles}

\setcounter{chapter}{4}
\begin{document}
\chapter{Least Square Problems}
\section{Introduction}
Let the set of data points be $(x_i,y_i), \;\;\; i=1, \, 2,\, ..., \, m$, and let the curve given by $y=f(x)$ be fitted to this data. If $e_i$ is the error of the approximation at $x=x_i$ due to this fitting then, $e_i= y_i - f(x_i)$. If we write $\displaystyle S= \sum_{i=1} ^ m e_i^2 =  \sum_{i=1} ^ m \; [y_i - f(x_i)]^2$. Then the method of minimizing this error which is the sum of the square of errors is called the method of least squares.

\subsection{Fitting a Straight Line}
Let $Y= a_0 + a_1x$ be the straight line to be fitted to the given data by the method of least squares. Then the sum of errors is $\displaystyle S = \sum_{i=1}^m \; [y_i - a_0 - a_1x_i]^2$. $S$ is a function of $a_0$ and $a_1$. To minimize $S$, $\displaystyle \frac{\partial S}{\partial a_0} = 0$ and $\displaystyle \frac{\partial S}{\partial a_1} = 0 $. We have,
\begin{equation}
  \label{eq:9}
  \begin{gathered}
    \frac{\partial S}{\partial a_0} = -2 \sum_{i=1}^m \; [y_i - a_0 - a_1x_i], \hspace{10mm}
     \frac{\partial S}{\partial a_1} = -2x_i \sum_{i=1}^m \; [y_i - a_0 - a_1x_i]
  \end{gathered}
\end{equation}
\begin{equation}
  \label{singlenormal1}
  \begin{aligned}[b]
  \frac{\partial S}{\partial a_0} = 0 &\implies -2 \sum_{i=1}^m \; [y_i - a_0 - a_1x_i] = 0 \\[1mm]
                                      &\implies \sum_{i=1}^m \; y_i = \sum_{i=1}^m \; a_0 + a_1 \sum_{i=1}^m \;x_i \\[1mm]
                                          &\implies \sum_{i=1}^m \; y_i = a_1 \sum_{i=1}^m \;x_i + ma_0
\end{aligned}
\end{equation}

\begin{equation}
  \label{singlenormal2}
\begin{aligned}[b]
  \frac{\partial S}{\partial a_1} = 0 &\implies -2x_i\sum_{i=1}^m \; [y_i - a_0 - a_1x_i] = 0 \\[1mm]
                                      &\implies \sum_{i=1}^m \; x_iy_i = a_0\sum_{i=1}^m \;x_i + a_1 \sum_{i=1}^m \;x_i^2
\end{aligned}
\end{equation}
The equations ~\ref{singlenormal1} and ~\ref{singlenormal2} are called normal equations which can be solved for $a_0$ and $a_1$. From the equation ~\ref{singlenormal1}, it can be easily shown that the fitted line passes through the \textit{means} of $x_i$ and $y_i$,i.e the line satisfies $\overline{y} = a_0 +a_1 \overline{x}$.

\subsection{Multiple Linear Least Square}
Let $z= a_0 + a_1x+a_2y$ be a linear equation to be fitted to the given data $(x_1,y_1,z_1), (x_2,y_2,z_2),...,(x_m,y_,z_m)$ by the method of least squares. Then the sum of errors is $\displaystyle S = \sum_{i=1}^m \; [z_i - a_0 - a_1x_i-a_2y_i]^2$. $S$ is a function of $a_0, a_1$ and $a_2$. To minimize $S$:
\begin{align*}
  \frac{\partial S}{\partial a_0} = -2 \sum_{i=1}^m \; [y_i - a_0 - a_1x_i-a_2y_i] = 0 \\[1mm]
  \frac{\partial S}{\partial a_1} = -2x_i \sum_{i=1}^m \; [y_i - a_0 - a_1x_i - a_2y_i] = 0 \\[1mm]
  \frac{\partial S}{\partial a_2} = -2y_i \sum_{i=1}^m \; [y_i - a_0 - a_1x_i - a_2y_i] = 0
\end{align*}
So, the normal equations are:
\begin{equation}
  \label{multinormal}
  \begin{aligned}[c]
    \sum_{i=1}^m \; z_i &= a_1 \sum_{i=1}^m \;x_i + a_2 \sum_{i=1}^m \;y_i + ma_0 \\[1mm]
    \sum_{i=1}^m \; z_i\,x_i &= a_1 \sum_{i=1}^m \;x_i^2 + a_2 \sum_{i=1}^m \;x_iy_i + a_0 \sum_{i=1}^m\; x_i \\[1mm]
    \sum_{i=1}^m \; z_i\,y_i &= a_1 \sum_{i=1}^m \;x_iy_i + a_2 \sum_{i=1}^m \;y_i^2 + a_0 \sum_{i=1}^m\; y_i
  \end{aligned}
\end{equation}

\section{Curve Fitting by Polynomials}
Let $Y = a_0 + a_1x +a_2x^2+ ...+ a_nx^n$ be a polynomial to be fitted to the given data $(x_1,y_1), (x_2,y_2),...,(x_m,y_m)$ by the method of least squares. Then the sum of errors is $\displaystyle S = \sum_{i=1}^m \; [z_i - a_0 - a_1x_i-a_2y_i]^2$. Then the normal equations are:
\begin{equation}
  \label{polynormal}
  \begin{aligned}[c]
    \sum_{i=1}^m \; y_i &= a_1 \sum_{i=1}^m \;x_i + a_2 \sum_{i=1}^m \;x_i^2 + ...+ ma_0 \\[1mm]
    \sum_{i=1}^m \; x_i\,y_i &= a_0\sum_{i=1}^m \;x_i + a_1 \sum_{i=1}^m \;x_i^2 + a_2 \sum_{i=1}^m \;x_i^3 + ... + a_n \sum_{i=1}^m\; x_i^{n+1} \\[1mm]
    ... \\[1mm]
    \sum_{i=1}^m \; x_i^n\,y_i &= a_0\sum_{i=1}^m \;x_i^n + a_1 \sum_{i=1}^m \;x_i^{n+1} + a_2 \sum_{i=1}^m \;x_i^{n+2} + ... + a_n \sum_{i=1}^m\; x_i^{2n}
  \end{aligned}
\end{equation}
This system constitutes of $(n+1)$ equations in $(n+1)$ unknowns. For larger value of $n$ the system is \textit{ill conditioned}, so orthogonal polynomials are used to fit such data points.

\section{Weighted Least Square Approximation}
For the errors are weighted by $W_i$, then the sum of weighed errors is: \\
$\displaystyle S= \sum_{i=1} ^ m\; W_i\, e_i^2 =  \sum_{i=1} ^ m \; W_i\, [y_i - f(x_i)]^2$. So that the normal equations of the linear curve fitting $Y=a_0 +a_1x$ becomes:
\begin{equation}
\begin{aligned}
  \sum_{i=1}^m \;W_i\, y_i &= a_1 \sum_{i=1}^m \;W_i\,x_i + ma_0 \\[1mm]
  \sum_{i=1}^m \; W_i\,x_iy_i &= a_0\sum_{i=1}^m \;xW_i\,_i + a_1 \sum_{i=1}^m \;W_i\,x_i^2
\end{aligned}
\end{equation}

\section{Linearization of Nonlinear Laws}
\begin{enumerate}
\item $\displaystyle y =ax + \frac{b}{x}$ \\[1mm]
This can be written as $xy = b + ax^2$. Put $xy=Y$, $b=A_0$ and $x^2 = X$. Then the given nonlinear equation becomes the linear equation $Y = A_0 + A_1X$.
\end{enumerate}

\item
\end{document}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
