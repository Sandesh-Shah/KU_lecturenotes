\documentclass[aima104_lecturenotes_ku.tex]{subfiles}

\setcounter{chapter}{3}
\begin{document}
\chapter{Orthogonality and Least Squares}
\section{Inner Product}
Let $u =
\begin{bmatrix}
  u_1 \\ u_2 \\ . \\ .\\ .\\ u_n
\end{bmatrix} $ and $v =
\begin{bmatrix}
  v_1 \\ v_2 \\ . \\ .\\ .\\ v_n
\end{bmatrix} $ be any two vectors in $\mathbb{R}^n$. Then the number $u^T\, v$ is called the \textbf{inner product} of $u$ and $v$. This inner product is also commonly known as \textbf{dot product} and denoted by $\mathbf{u.v}$.

\subsection{Properties of Inner Product}
\begin{mdframed}
  \begin{enumerate}
  \item $u.v = v.u$
  \item $(u+v).w = u.w + v.w$
  \item $(\alpha u).v = \alpha (u.v) = u. (cv)$
  \item $u.u \geq 0$ and $u.u = 0 \Longleftrightarrow u=0$
  \end{enumerate}
\end{mdframed}

\subsection{The Length of a Vector}
The length of a vector $v$ is called the \textbf{norm} of $v$. \\ It is denoted by $\Vert v \Vert$ and defined by $\Vert v \Vert = \sqrt{v_1^2 + v_2^2 + ... + v_n^2} $ so that, $\Vert v \Vert ^2 = v.v$ There are several kinds of norms actually, this particular norm is called \textbf{Euclidean norm}.
For any scalar $\alpha$, $\Vert \alpha v \Vert = \vert \alpha \vert \Vert v \Vert$. A vector whose length is unity is called a \textbf{unit vector}. If we divide a nonzero vector $v$ by its length, we obtain a unit vector $u$. This process is called \textbf{normalizing} of the vector $v$.

\subsubsection{Distance between vectors}
For $u$ and $v$ in a vector space $V$, the distance between them is written as dist(u,v) and is defined as $dist(u,v) = \Vert u-v \Vert$.

\section{Orthogonal Vectors}
The two vectors $u$ and $v$ are orthogonal vectors if their dot product is zero,i.e $u.v = 0$. Observe that the zero vector is orthogonal to every vector as $0^Tv=0$ for all $v$.
\begin{thm}[The Pythagorean Theorem]
  Two vectors $u$ and $v$ are orthogonal if and only if $\Vert u + v \Vert ^2 = \Vert u \Vert ^2 + \Vert v \Vert ^2$.
\end{thm}

\subsection{Orthogonal Complement}
\begin{itemize}
\item If a vector $z$ is orthogonal to every vectors in a subspace $W$ then, $z$ is said to be orthogonal to $W$.

\item The set of all vectors that are orthogonal to $W$ is called the \textbf{orthogonal complement} of $W$. It is denoted by $W^{\perp}$. $W^{\perp} = \{z \, : \forall v \in W \;  z.v=0 \} $
\end{itemize}

\begin{thm}
  \begin{enumerate}
  \item A vector $x$ is in $W^{\perp}$ if and only if $x$ is orthogonal to every vector in a set that is spans $W$.
  \item $W^{\perp}$ is also a subspace.
   \item Row space is orthogonal complement of the Null space for a matrix.
  \end{enumerate}
\end{thm}

\subsection{Orthogonal Sets}
A set of vectors $\{ u_1, ...u_p\}$ in a vector space $V$ is said to be \textbf{orthogonal set} if each pair of distinct vectors from the set is orthogonal,i.e for all $u_i, u_j \in V$ we have $u_i.u_j = 0$ whenever $i \neq j$.

\begin{definition}
  An orthogonal basis for a vector space $V$ is a basis for $V$ that is an orthogonal set.
\end{definition}

\begin{theorem}
  \label{orthopro}
  Let $\{u_1, ..., u_p\}$ be an orthogonal basis for a subspace $W$ of $\mathbb{R}^n$. For each $y$ in $W$, the coordinates of $y$ with respect to the orthogonal basis : $y = c_1u_1 + \; ... \; c_n u_n$ are given by $\displaystyle c_j = \frac{y.u_j}{u_j.u_j}$.
\end{theorem}

\subsection{Orthogonal Projection}
The coordinate $c_j$ of $y$ in Theorem~\ref{orthopro} is actually orthogonal projection of $y$ into the vector $u_j$. This can be generalized. For any given vector $u$. The orthogonal projection of a vector $y$ on $u$ is given by the formula $\displaystyle \hat{y} = \frac{y.u}{u.u}$

Or, it can be derived as follows using the inner-product. $(y-\alpha u)$ and $u$ are orthogonal so, $(y-\alpha u).u=0$. This gives us $\displaystyle \alpha = \frac{y.u}{u.u}$. For two dimensional vectors, another orthogonal component $z$ can be easily obtained as by subtracting the projection from the vector $y$. $z = y - \hat{y}$.

\section{Orthonormal Sets}
A set $\{u_1, ..., u_p\}$ is an orthonormal set if it is an orthogonal set of unit vectors. And a basis of orthonormal set is a orthonormal basis. The simplest orthonormal basis is $\{e_1, \; ...\; e_n$ for $\mathbb{R}^n$.

Matrices whose columns form an orthonormal set are important in applications and in computer algorithms for matrix computations.

\begin{theorem}
  An $m \times n$ matrix $U$ has orthonormal columns if and only if $U^tU=I$
\end{theorem}

\end{document}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: t
%%% End:
